{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_results(data_dict, x_axis, title=None, xlabel=None, ylabel=None, loc=1):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.gca()\n",
    "    ax.set_xticks(x_axis)\n",
    "    plt.xlim(x_axis[0]-0.1, x_axis[-1]+0.1)\n",
    "    if title is not None: plt.title(title)\n",
    "    for label,data in data_dict.items():\n",
    "        plt.plot(x_axis, data, 'o-', label=label)\n",
    "    plt.legend(loc=loc)\n",
    "    if xlabel is not None: plt.xlabel(xlabel)\n",
    "    if ylabel is not None: plt.ylabel(ylabel)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se utilizaran modelos de Autoencoders en tres escenarios clásicos: reducción de dimensionalidad, denoising y pre-entrenamiento. La implementación será efectuada sobre un dataset denominado MNIST, una colección de 70.000 imágenes de 28 × 28 pixeles correspondientes a dígitos manuscritos (números entre 0 y 9). En su versión tradicional, la colección se encuentra separada en dos subconjuntos: uno de entrenamiento de 60.000 imágenes y otro de test de 10.000 imágenes. Es de nuestro interés entrenar un programa para que aprenda a identificar correctamente el dígito representado en la imagen.\n",
    "\n",
    "*** a)*** Inicialmente se cargan los datos desde el repositorio de keras, y se normalizan las imágenes de modo que los pixeles queden en $[0, 1]$. Tal normalización permite interpretar cada valor como la probabilidad de que cada pixel sea activado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizacion de imagenes\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se transforman las imágenes en vectores ($\\in \\mathbb{R}^{784}$) y se generan tres subconjuntos disjuntos: uno de entrenamiento, uno de validación y uno de pruebas. El conjunto de validación corresponde a los últimos 5000 casos del conjunto del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformación en vectores R^{784}\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Creación conjunto de validación\n",
    "nval = 5000\n",
    "x_val = x_train[-nval:]\n",
    "y_val = y_train[-nval:]\n",
    "x_train = x_train[:-nval]\n",
    "y_train = y_train[:-nval]\n",
    "\n",
    "# Transformación de salidas a probabilidades de activación\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_val = np_utils.to_categorical(y_val, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Reducción de Dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las aplicaciones típicas de un AE es reducción de dimensionalidad, es decir, implementar una transformación $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d'}$, con $d'<< d$ que preserve lo mejor posible la información original. Obtener tal representación es útil desde un punto de vista computacional (compresión) y estadístico (permite construir modelos con un\n",
    "menor número de parámetros libres). Un AE es una técnica de reducción de dimensionalidad no supervisada porque no hace uso de información acerca de las clases a las que pertenecen los datos de entrenamiento.\n",
    "\n",
    "*** a) *** A continuación, se entrena un AE básico (1 capa escondida) para generar una representación de MNIST en $d' = 2, 8, 32, 64$ dimensiones. Inicialmente se define un error cuadrático de reconstrucción ya que en secciones posteriores se requiere comparar los resultados con PCA, el cual no necesariamente mantiene sus predicciones en el intervalo  $[0,1]$ exigido para un error de reconstrucción binary cross entropy. Además, el uso de un error de reconstrucción binary cross entropy se consideraría un abuso del modelo ya que este se define en primera instancia para input $\\in \\{0,1\\}$ y no necesariamente dentro de su intervalo. Cabe destacar, que tal error de recostrucción es sólo para efectos de evaluación de modelos, ya que la función de pérdida que efectivamente interviene durante el aprendizaje sí es binary cross entropy, como es habitual para casos de entrada dentro del intervalo $[0,1]$ con salidas independientes entre sí, deducido por el método de máxima verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_reconstruccion(x_test, x_decoded_test):\n",
    " \n",
    "    error_reco = 0\n",
    "    for l in range(len(x_test)): # para cada vector de prueba\n",
    "        error = 0\n",
    "        for i in range(len(x_test[0])): # para cada salida\n",
    "            error += (x_test[l,i] - x_decoded_test[l,i])**2\n",
    "    error_reco += error\n",
    "    return error_reco/len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se determina el porcentaje de comprensión obtenido y el error de reconstrucción en cada caso. Además se consideran los siguientes dos casos para las funciones de activación utilizadas:\n",
    "\n",
    "_i) Encoder y decoder: Sigmoidal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.4295 - val_loss: 0.3277\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.3056 - val_loss: 0.2918\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2854 - val_loss: 0.2807\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2778 - val_loss: 0.2755\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2738 - val_loss: 0.2725\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2714 - val_loss: 0.2706\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2699 - val_loss: 0.2693\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2687 - val_loss: 0.2683\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2679 - val_loss: 0.2676\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2673 - val_loss: 0.2671\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2668 - val_loss: 0.2666s\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2663 - val_loss: 0.2662\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2660 - val_loss: 0.2659\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2657 - val_loss: 0.2657\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2655 - val_loss: 0.2655\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2653 - val_loss: 0.2653\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2651 - val_loss: 0.2651\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2649 - val_loss: 0.2650\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2648 - val_loss: 0.2648\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2647 - val_loss: 0.2647\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2646 - val_loss: 0.2646\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2645 - val_loss: 0.2645\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2644 - val_loss: 0.2644\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2643 - val_loss: 0.2644\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2642 - val_loss: 0.2643\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2642 - val_loss: 0.2642\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2641 - val_loss: 0.2642\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2641 - val_loss: 0.2641\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2640 - val_loss: 0.2641\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2639 - val_loss: 0.2640\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2639 - val_loss: 0.2640\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2639 - val_loss: 0.2639\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2638 - val_loss: 0.2639\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2638 - val_loss: 0.2639\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2638 - val_loss: 0.2638\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2637 - val_loss: 0.2638\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2637 - val_loss: 0.2638\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2637 - val_loss: 0.2637\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2636 - val_loss: 0.2637\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2636 - val_loss: 0.2637\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2636 - val_loss: 0.2637\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2636 - val_loss: 0.2637\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2634 - val_loss: 0.2635\n",
      "d'= 2\n",
      "Porcentaje de compresion: 0.25510204081632654 %\n",
      "Error de reconstruccion: 0.00776495413464\n",
      "\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.3459 - val_loss: 0.2810\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2745 - val_loss: 0.2707\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2689 - val_loss: 0.2677\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2668 - val_loss: 0.2663\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2658 - val_loss: 0.2655\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 13s - loss: 0.2651 - val_loss: 0.2650\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 12s - loss: 0.2647 - val_loss: 0.2647\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2644 - val_loss: 0.2644\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 14s - loss: 0.2642 - val_loss: 0.2642\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2640 - val_loss: 0.2640\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2639 - val_loss: 0.2639\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2638 - val_loss: 0.2638\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2637 - val_loss: 0.2637\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2636 - val_loss: 0.2637\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 12s - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 13s - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2634 - val_loss: 0.2634\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2633 - val_loss: 0.2634\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 15s - loss: 0.2633 - val_loss: 0.2634\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 13s - loss: 0.2633 - val_loss: 0.2634\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 14s - loss: 0.2632 - val_loss: 0.2633\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 12s - loss: 0.2632 - val_loss: 0.2633\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 12s - loss: 0.2632 - val_loss: 0.2633\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2632 - val_loss: 0.2633\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2632 - val_loss: 0.2633\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 15s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 13s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000/55000 [==============================] - 11s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 11s - loss: 0.2631 - val_loss: 0.2632\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2632\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 10s - loss: 0.2630 - val_loss: 0.2631\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 9s - loss: 0.2630 - val_loss: 0.2631\n",
      "d'= 8\n",
      "Porcentaje de compresion: 1.0204081632653061 %\n",
      "Error de reconstruccion: 0.00778372243456\n",
      "\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 15s - loss: 0.2965 - val_loss: 0.2670\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 17s - loss: 0.2655 - val_loss: 0.2647\n",
      "Epoch 3/50\n",
      "42750/55000 [======================>.......] - ETA: 2s - loss: 0.2644"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-64eb9eee5bd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mencoded_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mdecoded_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# Dimensiones\n",
    "dim = np.array([2, 8, 32, 64])\n",
    "\n",
    "# Para guardar los modelos\n",
    "autoencoder = [None]*4\n",
    "encoder = [None]*4\n",
    "decoder = [None]*4\n",
    "encoded_test = [None]*4\n",
    "decoded_test = [None]*4\n",
    "scores = np.empty(4)\n",
    "\n",
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()\n",
    "\n",
    "for i in range(4):\n",
    "    # AE para cada dimension\n",
    "    encoded = Dense(dim[i], activation='sigmoid')(input_img)\n",
    "    decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "    autoencoder[i] = Model(input=input_img, output=decoded)\n",
    "    encoder[i] = Model(input=input_img, output=encoded)\n",
    "    encoded_input = Input(shape=(dim[i],))\n",
    "    decoder_layer = autoencoder[i].layers[-1]\n",
    "    decoder[i] = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "    autoencoder[i].compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    autoencoder[i].fit(x_train, x_train, nb_epoch=50, batch_size=25, shuffle=True, verbose=1,validation_data=(x_val, x_val))\n",
    "    encoded_test[i] = encoder[i].predict(x_test)\n",
    "    decoded_test[i] = decoder[i].predict(encoded_test[i])\n",
    "    error = error_reconstruccion(x_test, decoded_test[i])\n",
    "    print (\"d'= \" + str(dim[i]))\n",
    "    print (\"Porcentaje de compresion: \" + str(float(dim[i])*100/784) +\" %\")\n",
    "    print (\"Error de reconstruccion: \" + str(error))\n",
    "    print ('')\n",
    "    autoencoder[i].save('entrenamientos/basic_autoencoder_768x' + str(dim[i]) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ii) Encoder: ReLu, decoder: Sigmoidal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IDEM pero con ReLu\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# Dimensiones\n",
    "dim = np.array([2, 8, 32, 64])\n",
    "\n",
    "# Para guardar los modelos\n",
    "r_autoencoder = [None]*4\n",
    "r_encoder = [None]*4\n",
    "r_decoder = [None]*4\n",
    "r_encoded_test = [None]*4\n",
    "r_decoded_test = [None]*4\n",
    "r_scores = np.empty(4)\n",
    "\n",
    "for i in range(4):\n",
    "    # AE para cada dimension\n",
    "    encoded = Dense(dim[i], activation='relu')(input_img)\n",
    "    decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "    r_autoencoder[i] = Model(input=input_img, output=decoded)\n",
    "    r_encoder[i] = Model(input=input_img, output=encoded)\n",
    "    encoded_input = Input(shape=(dim[i],))\n",
    "    decoder_layer = r_autoencoder[i].layers[-1]\n",
    "    r_decoder[i] = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "    r_autoencoder[i].compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    r_autoencoder[i].fit(x_train,x_train,nb_epoch=50,batch_size=25,shuffle=True, #nb_epoch=50, batch_size=25\n",
    "                    verbose=1,validation_data=(x_val, x_val))\n",
    "    r_encoded_test[i] = r_encoder[i].predict(x_test)\n",
    "    r_decoded_test[i] = r_decoder[i].predict(r_encoded_test[i])\n",
    "    #scores[i] = autoencoder[i].evaluate(decoded_test[i], x_test, verbose=0)\n",
    "    error = error_reconstruccion(x_test, r_decoded_test[i])\n",
    "    print (\"d'= \" + str(dim[i]))\n",
    "    print (\"Porcentaje de compresion: \" + str(float(dim[i])*100/784) +\" %\")\n",
    "    print (\"Error de reconstruccion: \" + str(error))\n",
    "    print ('')\n",
    "    r_autoencoder[i].save('entrenamientos/relu_basic_autoencoder_768x' + str(dim[i]) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presenta un gráfico resumen de los resultados de error de reconstrucción obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Almacenando resultados en listas\n",
    "rec_err_ae_sigmoid = [0.00776104132228, 0.00704934253329, 0.00328040257189, 0.00203037510476]\n",
    "rec_err_ae_relu = [0.00617615486544, 0.00293794734278, 0.00116055652994, 0.00052300655125]\n",
    "\n",
    "data_dict = {'RecErr AE sigmoid': rec_err_ae_sigmoid, 'RecErr AE relu':rec_err_ae_relu}\n",
    "show_results(data_dict, [2,8,32,64], xlabel=\"d'\", ylabel='RecErr', title=\"Reconstruction error v/s d'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es de esperarse para ambos casos de función de activación del encoder, la capacidad de reconstrucción aumenta con valores de `d'` mayores, ya que las representaciones encontradas permiten conservar mayor información del patrón de entrada. Además, se evidencia una mejora significativa al cambiar la función de activación por `relu`, sugiriendo una mejor capacidad de representación en baja dimensionalidad. En general, el nivel de dispersión en las representaciones generadas por ReLu favorece la eficiencia al reproducir entradas con tamaños variables, esto debido a que variando el número de neuronas activas permite al modelo controlar la efectiva dimensionalidad de la representación para una imagen dada. En términos cuantitativos ambas funciones de activación ofrecen un bajo error de reconstrucción, sin embargo ReLu permite reducirlo a valores cercanos a la mitad de los obtenidos con Sigmoid. Notar que cuando la comprensión es muy alta ($d'=2$) las representaciones tienen menos capacidad de diferenciación entre sí, lo cual es rápidamente diluido al aumentar $d'$. Análogamente cuando la comprensión es despreciable, ambos errores (para cada función de activación) debiesen asemejarse dado lo poco restrictivo del encoder.\n",
    "\n",
    "Con respecto al uso de ReLU para el decoder, no parece indicado dado el comportamiento ilimitado de la función para argumentos positivos, lo cual hiere la representación de pixeles normalizada al intervalo $[0,1]$. Además, la fuerte saturación en cero de ReLu, obliga a que cada vez que la red reconstruya un cero en vez de un valor correcto distinto de cero, dicha unidad de reconstrucción no podrá propagar ningún gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** b)*** Ahora se verifica visualmente la calidad de las representaciones $\\mathbb{R}^{d'}$ que logra hacer el autoencoder desde la representación en $\\mathbb{R}^{d}$. Para esto se hace uso de las primeras diez imágenes del conjunto de pruebas.\n",
    "\n",
    "_i) Encoder y decoder: Sigmoidal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load models\n",
    "\n",
    "for j in range(4):\n",
    "    autoencoder = load_model('entrenamientos/basic_autoencoder_768x' + str(dim[j]) + '.h5')\n",
    "\n",
    "    encoded_test = encoder[j].predict(x_test)\n",
    "    decoded_test = decoder[j].predict(encoded_test)\n",
    "    print(encoded_test.shape)\n",
    "\n",
    "    # plots\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    \n",
    "    print (\"Reconstruccion para d'=\" + str(dim[j]))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ii) Encoder: ReLu, decoder: Sigmoidal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IDEM pero con ReLu\n",
    "\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load models\n",
    "\n",
    "for j in range(4):\n",
    "    autoencoder = load_model('entrenamientos/relu_basic_autoencoder_768x' + str(dim[j]) + '.h5')\n",
    "    encoded_test = r_encoder[j].predict(x_test)\n",
    "    decoded_test = r_decoder[j].predict(encoded_test)\n",
    "    print(encoded_test.shape)\n",
    "\n",
    "    # plots\n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    \n",
    "    print (\"Reconstruccion para d'=\" + str(dim[j]))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_test[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se comprueba cualitativamente los resultados obtenidos en b). Se aprecia para ambas funciones de activación una fuerte mejoría al diminuir el porcentaje de comprensión. En cuanto a las diferencias observadas entre ReLu y Sigmoid, el mejor desempeño de la primera es evidenciado para $d'=8$ (donde cuantitativamente se produjo la mayor brecha en el error de reconocimiento), donde Sigmoid arroja representaciones irreconocibles, mientras que ReLu ya ofrece representaciones bastante precisas. Además, la diferencia de nitidez para $d'=64$ para ambas funciones es concluyente.\n",
    "\n",
    "***c)*** Para verificar la calidad de la representación obtenida, se implementa el siguiente clasificador denominado $kNN$ (k-nearest neighbor): dada una imagen $\\textbf{x}$, el clasificador busca las $k = 10$ imágenes de entrenamiento más similares $N_{\\textbf{x}} = \\{\\textbf{x}^{(k_i)}\\}^{10}_{i=1}$ (de acuerdo a una distancia, e.g. euclidiana) y predice como clase, la etiqueta más popular entre las imágenes $N_x$. Se crea la siguiente función para el calculo de accuracy y tiempo de las predicciones: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "def knn_classification_accuracy(X_train, y_train, X_test, y_test):\n",
    "    clf = KNeighborsClassifier(10)\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    elapsed = time.time()-t0\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print('Classification Accuracy: %.2f' % score)\n",
    "    print('Elapsed time: {0}[s] \\n'.format(elapsed))\n",
    "    del clf\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la data original (previa a la reducción):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Data original')\n",
    "knn_classification_accuracy(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las clasificaciones sobre la data reducida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Sigmoid + d=2')\n",
    "knn_classification_accuracy(encoder[0].predict(x_train), y_train, encoder[0].predict(x_test), y_test)\n",
    "\n",
    "print('Sigmoid + d=8')\n",
    "knn_classification_accuracy(encoder[1].predict(x_train), y_train, encoder[1].predict(x_test), y_test)\n",
    "\n",
    "print('Sigmoid + d=32')\n",
    "knn_classification_accuracy(encoder[2].predict(x_train), y_train, encoder[2].predict(x_test), y_test)\n",
    "\n",
    "print('Sigmoid + d=64')\n",
    "knn_classification_accuracy(encoder[3].predict(x_train), y_train, encoder[3].predict(x_test), y_test)\n",
    "\n",
    "print('ReLU + d=2')\n",
    "knn_classification_accuracy(r_encoder[0].predict(x_train), y_train, r_encoder[0].predict(x_test), y_test)\n",
    "\n",
    "print('ReLU + d=8')\n",
    "knn_classification_accuracy(r_encoder[1].predict(x_train), y_train, r_encoder[1].predict(x_test), y_test)\n",
    "\n",
    "print('ReLU + d=32')\n",
    "knn_classification_accuracy(r_encoder[2].predict(x_train), y_train, r_encoder[2].predict(x_test), y_test)\n",
    "\n",
    "print('ReLU + d=64')\n",
    "knn_classification_accuracy(r_encoder[3].predict(x_train), y_train, r_encoder[3].predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto de los tiempo de entrenamiento del clasificador kNN, se aprecia un aumento significativo al incrementar dimensionalidad de la data. Esto de debe a que kNN es un algoritmo de complejidad $O(ndk)$, siendo $k$ un hiperparámetro fijo, $n$ la cantidad de imágenes recibidas y $d$ la dimensionalidad de cada input, ya que requiere computar las distancias entre todos los ejemplos de entrenamiento, cálculo que es más complejo a mayor dimensionalidad. Así, es de esperarse que al considerar la data con dimensionalidad no reducida, el tiempo de clasificación es proporcionalmente mayor con respecto a las diferencias de dimensionalidades ($784-64$ en el caso más comparable).\n",
    "\n",
    "Luego, se procede a graficar la accuracy de clasificación en función de la dimensionalidad exigida por el encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Almacenando resultados en listas\n",
    "ca_ae_sigmoid = [0.15, 0.31, 0.87, 0.96]\n",
    "ca_ae_relu = [0.42, 0.89, 0.96, 0.96]\n",
    "data_dict = {'CA AE sigmoid': ca_ae_sigmoid, 'CA AE relu':ca_ae_relu}\n",
    "show_results(data_dict, [2,8,32,64], xlabel='d', ylabel='Acc', title='Classification Accuracy v/s d', loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son consistentes a los obtenidos en secciones anteriores, donde la capacidad de clasificación significa representaciones generadas competentes, las cuales debiesen mejorar a medida que el patrón original se comprime en menor medida. Si bien ambas curvas alcanzan su máximo de presición en 0.96, la figura es concluyente sobre el mejor desempeño de ReLu en representaciones de baja dimensionalidad. Notar que la diferencia de accuracy entre una comprensión de dimensionalidad 64 y el input original de dimensionalidad 784 varían relativamente poco (en 0.01) comparado con la fuerte diferencia temporal en la clasificación, lo cual evidencia las representaciones idóneas generadas por los AE's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*** d) *** Ahora se compara la calidad de la representación reducida obtenida por el autoencoder básico con aquella obtenida vía PCA utilizando el mismo número de dimensiones $d'=2,8,32,64$. Se considera como criterio de comparación el error de reconstrucción y desempeño en clasificación (vía kNN) de cada representación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pca = [None]*4 # guardar modelos PCA\n",
    "pca_train = [None]*4 # Guardar PCA train\n",
    "pca_test = [None]*4 # Guardar PCA test\n",
    "pca_decoded = [None]*4\n",
    "\n",
    "# Dimensiones\n",
    "dim = np.array([2, 8, 32, 64])\n",
    "\n",
    "for i in range(4): # para cada dimension d'\n",
    "    pca[i] = PCA(n_components=dim[i])\n",
    "    pca[i].fit(x_train)\n",
    "    pca_train[i] = pca[i].transform(x_train)\n",
    "    pca_test[i] = pca[i].transform(x_test)\n",
    "    pca_decoded[i] = pca[i].inverse_transform(pca_test[i])\n",
    "    clf = KNeighborsClassifier(10)\n",
    "    clf.fit(pca_train[i], y_train)\n",
    "    score = clf.score(pca_test[i],y_test)\n",
    "    error = error_reconstruccion(x_test, pca_decoded[i])\n",
    "    \n",
    "    print (\"Representacion para d'=%.d\" % dim[i])\n",
    " \n",
    "    print (\"Error de reconstruccion: \" + str(error))\n",
    "    print ('Classification Accuracy %.2f' % score)\n",
    "\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error de reconstrucción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_err_pca = [0.00609441194644, 0.00280048892998, 0.00189737605735, 0.00102204343252]\n",
    "data_dict = {'RecErr AE sigmoid': rec_err_ae_sigmoid, 'RecErr AE relu':rec_err_ae_relu, 'RecErr PCA':rec_err_pca}\n",
    "show_results(data_dict, [2,8,32,64], xlabel='d', ylabel='RecErr', title='Reconstruction error v/s d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del gráfico se desprende que la capacidad de reconstrucción de PCA obtuvo mejores resultados que AE con Sigmoid, pero quedó por debajo del desempeño de ReLu al considerar dimensiones de representación mayores a $d'=32$. En cuanto a valores más pequeños de $d'$, la representación generada con PCA obtiene errores de reconstrucción incluso ligeramente menores a los obtenidos con el AE ReLu. Esto es bastante notable, teniendo en cuenta que PCA tan sólo realiza una transformación lineal sobre los datos, mientras que los AEs relizan transformaciones no lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Desempeño en clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ca_pca = [0.44, 0.90, 0.97, 0.97] \n",
    "data_dict = {'CA AE sigmoid': ca_ae_sigmoid, 'CA AE relu':ca_ae_relu, 'CA PCA':ca_pca}\n",
    "show_results(data_dict, [2,8,32,64], xlabel='d', ylabel='Acc', title='Classification Accuracy v/s d', loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto al proceso de clasificación vía kNN, la representación generada por PCA consigue un accuracy muy similar pero ligeramente mayor que AE con ReLu en todo valor de $d'$ estudiado, hasta llegar a un desempeño de predicción igual al encontrado con la data antes de ser reducida (accuracy: 0.97). Se entiende así, que el accuracy logrado por la representación del AE con Sigmoid es cómodamente superada con valores de $d'$ más restrictivos, condición equilibrada a medida que la dimensionalidad aumenta. \n",
    "\n",
    "Se destaca además que el tiempo invertido en el uso de PCA es mucho menor que cuando se prueban los autoencoders.\n",
    "\n",
    "*** e) *** Ahora se modifica el autoencoder básico construido en a) para implementar un *deep autoencoder* (deep AE), es decir, un autoencoder con al menos dos capas ocultas. Esto con el fin de demostrar experimentalmente que este autoencoder puede mejorar significativamente la compresión obtenida por PCA utilizando el mismo número de dimensiones $d'$. Se expermienta con $d' = 2, 4, 8, 16, 32$ y distintas profundidades $(L = 2, 3, 4)$. Se utiliza solo activación ReLu en el encoder, ya que por razones de tiempo se privilegió aquella que hasta ahora a mostrado mejor desempeño. Nuevamente se considera como criterio de comparación el error de reconstrucción y desempeño en clasificación (vía kNN) de cada representación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "\n",
    "import numpy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "autoencoder4 = [None]*5\n",
    "autoencoder3 = [None]*5\n",
    "autoencoder2 = [None]*5\n",
    "encoder4 = [None]*5\n",
    "encoder3 = [None]*5\n",
    "encoder2 = [None]*5\n",
    "decoded_test4 = [None]*5\n",
    "decoded_test3 = [None]*5\n",
    "decoded_test2 = [None]*5\n",
    "encoded_test4 = [None]*5\n",
    "encoded_test3 = [None]*5\n",
    "encoded_test2 = [None]*5\n",
    "encoded_train4 = [None]*5\n",
    "encoded_train3 = [None]*5\n",
    "encoded_train2 = [None]*5\n",
    "\n",
    "pca = [None]*5 \n",
    "pca_train = [None]*5 \n",
    "pca_test = [None]*5  \n",
    "pca_decoded = [None]*5\n",
    "\n",
    "for i in range(5):\n",
    "    target_dim = 2**(i+1)\n",
    "    \n",
    "    encoded1 = Dense(1000, activation='relu')(input_img)\n",
    "    encoded2 = Dense(500, activation='relu')(encoded1)\n",
    "    encoded3 = Dense(250, activation='relu')(encoded2)\n",
    "    encoded4 = Dense(target_dim, activation='relu')(encoded3)\n",
    "    decoded4 = Dense(250, activation='relu')(encoded4)\n",
    "    decoded3 = Dense(500, activation='relu')(encoded3)\n",
    "    decoded2 = Dense(1000, activation='relu')(decoded3)\n",
    "    decoded1 = Dense(784, activation='sigmoid')(decoded2)\n",
    "    autoencoder4[i] = Model(input=input_img, output=decoded1)\n",
    "    encoder4[i] = Model(input=input_img, output=encoded3)\n",
    "    autoencoder4[i].compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    autoencoder4[i].fit(x_train,x_train,nb_epoch=20,batch_size=25,shuffle=True,validation_data=(x_val, x_val), verbose=0)\n",
    "    decoded_test4[i] = autoencoder4[i].predict(x_test)\n",
    "    error = error_reconstruccion(x_test, decoded_test4[i])\n",
    "    encoded_train4[i] = encoder4[i].predict(x_train)\n",
    "    encoded_test4[i] = encoder4[i].predict(x_test)\n",
    "    # Clasificador kNN\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    clf = KNeighborsClassifier(10)\n",
    "    clf.fit(encoded_train4[i], y_train)\n",
    "    scorekNN = clf.score(encoded_test4[i],y_test)\n",
    "    print (\"d'= \" + str(target_dim) + \" con 4 capas ocultas\") \n",
    "    print (\"Error de reconstruccion: \" + str(error))\n",
    "    print ('Classification Accuracy %.2f' % scorekNN) \n",
    "    print (\" \")\n",
    "    \n",
    "    encoded1 = Dense(1000, activation='relu')(input_img)\n",
    "    encoded2 = Dense(500, activation='relu')(encoded1)\n",
    "    encoded4 = Dense(target_dim, activation='relu')(encoded2)\n",
    "    decoded3 = Dense(500, activation='relu')(encoded4)\n",
    "    decoded2 = Dense(1000, activation='relu')(decoded3)\n",
    "    decoded1 = Dense(784, activation='sigmoid')(decoded2)\n",
    "    autoencoder3[i] = Model(input=input_img, output=decoded1)\n",
    "    encoder3[i] = Model(input=input_img, output=encoded2)\n",
    "    autoencoder3[i].compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    autoencoder3[i].fit(x_train,x_train,nb_epoch=20,batch_size=25,shuffle=True,validation_data=(x_val, x_val), verbose=0)\n",
    "    decoded_test3[i] = autoencoder3[i].predict(x_test)\n",
    "    error = error_reconstruccion(x_test, decoded_test3[i])\n",
    "    encoded_train3[i] = encoder3[i].predict(x_train)\n",
    "    encoded_test3[i] = encoder3[i].predict(x_test)\n",
    "    # Clasificador kNN\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    clf = KNeighborsClassifier(10)\n",
    "    clf.fit(encoded_train3[i], y_train)\n",
    "    scorekNN = clf.score(encoded_test3[i],y_test)\n",
    "    print (\"d'= \" + str(target_dim) + \" con 3 capas ocultas\") \n",
    "    print (\"Error de reconstruccion: \" + str(error))\n",
    "    print ('Classification Accuracy %.2f' % scorekNN) \n",
    "    print (\" \")\n",
    "    \n",
    "    encoded1 = Dense(1000, activation='relu')(input_img)\n",
    "    encoded4 = Dense(target_dim, activation='relu')(encoded1)\n",
    "    decoded2 = Dense(1000, activation='relu')(encoded4)\n",
    "    decoded1 = Dense(784, activation='sigmoid')(decoded2)\n",
    "    autoencoder2[i] = Model(input=input_img, output=decoded1)\n",
    "    encoder2[i] = Model(input=input_img, output=encoded1)\n",
    "    autoencoder2[i] = Model(input=input_img, output=decoded1)\n",
    "    encoder2[i] = Model(input=input_img, output=encoded3)\n",
    "    autoencoder2[i].compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    autoencoder2[i].fit(x_train,x_train,nb_epoch=20,batch_size=25,shuffle=True,validation_data=(x_val, x_val), verbose=0)\n",
    "    decoded_test2[i] = autoencoder2[i].predict(x_test)\n",
    "    error = error_reconstruccion(x_test, decoded_test2[i])\n",
    "    encoded_train2[i] = encoder2[i].predict(x_train)\n",
    "    encoded_test2[i] = encoder2[i].predict(x_test)\n",
    "    # Clasificador kNN\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    clf = KNeighborsClassifier(10)\n",
    "    clf.fit(encoded_train2[i], y_train)\n",
    "    scorekNN = clf.score(encoded_test2[i],y_test)\n",
    "    print (\"d'= \" + str(target_dim) + \" con 2 capas ocultas\") \n",
    "    print (\"Error de reconstruccion: \" + str(error))\n",
    "    print ('Classification Accuracy %.2f' % scorekNN) \n",
    "    print (\" \")\n",
    "    \n",
    "    pca[i] = PCA(n_components=target_dim)\n",
    "    pca[i].fit(x_train)\n",
    "    pca_train[i] = pca[i].transform(x_train)\n",
    "    pca_test[i] = pca[i].transform(x_test)\n",
    "    pca_decoded[i] = pca[i].inverse_transform(pca_test[i])\n",
    "    clf = KNeighborsClassifier(10)\n",
    "    clf.fit(pca_train[i], y_train)\n",
    "    score = clf.score(pca_test[i],y_test)\n",
    "    error = error_reconstruccion(x_test, pca_decoded[i])\n",
    "    print (\"Representacion PCA para d'=%.d\" % target_dim)\n",
    "    print (\"Error de reconstruccion: \" + str(error))\n",
    "    print ('Classification Accuracy %.2f' % score)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error de reconstrucción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_err_dae_2L=[0.00499643001335, 0.00218255287115, 0.00130909572334, 0.00123277115973, 0.000878753908421]\n",
    "rec_err_dae_3L=[0.0039078474435, 0.00275380790175, 0.0012719846182, 0.00113749076251,0.000937376116225]\n",
    "rec_err_dae_4L=[0.000487121992576, 0.000610392727038, 0.00053531392638, 0.000469686245258, 0.000415916509379]\n",
    "rec_err_pca=[0.0060943942994, 0.00427755649514, 0.00279948081487, 0.00250184787486, 0.00189068929]\n",
    "data_dict = {'PCA': rec_err_pca, 'Deep AE L=2': rec_err_dae_2L, 'Deep AE L=3': rec_err_dae_3L, 'Deep AE L=4': rec_err_dae_4L}\n",
    "show_results(data_dict, [2,4,8,16,32], xlabel='d', ylabel='RecErr', title='Reconstruction error v/s d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, es posible concluir empíricamente la superioridad de _deep AE's_ sobre PCA con respecto al error de reconstrucción cuadrático. PCA es superado en todas las dimensionalidades de representaciones y por todos los modelos _deep_ utilizados. Si bien, aumentar la cantidad de capas del AE no garantiza mejor calidad de las representaciones, en este caso si existe tendencia a este comportamiento, exceptuándo en algunos casos de ligera superioridad de $L=2$ por sobre $L=3$. Notar además, que ya para el AE más profundo $L=4$, una mayor comprensión no representa una penalización considerable en el error de reconstrucción, sugiriendo así, que AE profundos son capaces de generar ricas representaciones del patrón de entrada incluso a dimensionalidades muy restringidas (como $d'=2$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ca_dae_2L=[0.97, 0.97, 0.97, 0.97, 0.97]\n",
    "ca_dae_3L=[0.92, 0.94, 0.96, 0.98, 0.97]\n",
    "ca_dae_4L=[0.97, 0.97, 0.97, 0.97, 0.97]\n",
    "ca_pca=[0.44, 0.64, 0.90, 0.96, 0.97]\n",
    "data_dict = {'PCA': ca_pca, 'Deep AE L=2': ca_dae_2L, 'Deep AE L=3': ca_dae_3L, 'Deep AE L=4': ca_dae_4L}\n",
    "show_results(data_dict, [2,4,8,16,32], xlabel='d', ylabel='Acc', title='Classification Accuracy v/s d', loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En cuanto al accuracy en clasificación, vuelven a encontrarse mejores desempeños para _deep's_ autoencoders por sobre representaciones obtenidas vía PCA. Con $L=2$ y $L=4$ se observan resultados similares (la curva naranja está bajo la roja). Para $L=3$ se consigue una pequeña diferencia de accuracy con el resto de sus semejantes para menores dimensiones, la cual es acortada en $d'=16$. Sin embargo, las brechas se vuelven considerables al comparar _deep's_ AE's con PCA en $d'=2$, $d'=4$ y $d'=8$, donde este último obtiene accuracy's de clasificación muy deficientes, las cuales recién resultan competitivas y comparables con  _deep's_ AE's en $d'=16$, desde donde las diferencias se reducen al encontrar representaciones del patrón de entrada de calidad similar.\n",
    "\n",
    "***f)*** A continuación se visualizan las representaciones aprendidas anteriormente usando la herramienta TSNE disponible en la librería _sklearn_. Esta herramienta convierte similitudes entre puntos de datos a probabilidades conjuntas e intenta minimizar la divergencia de Kullback-Leibler entre dichas probabilidades conjuntas del _embedding_ de baja dimensión y los datos de alta dimensionalidad. Es de nuestro particular interés evidenciar las diferencias obtenidas por el _deep_ AE con $L=4$ y PCA al generar representaciones con $d'=2$. Adicionalmente se estudiará el caso $d'=16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "target_dim = 2 #try other and do a nice plot\n",
    "input_img = Input(shape=(784,))\n",
    "encoded1 = Dense(1000, activation='relu')(input_img)\n",
    "encoded2 = Dense(500, activation='relu')(encoded1)\n",
    "encoded3 = Dense(250, activation='relu')(encoded2)\n",
    "encoded4 = Dense(target_dim, activation='relu')(encoded3)\n",
    "decoded4 = Dense(250, activation='relu')(encoded4)\n",
    "decoded3 = Dense(500, activation='relu')(encoded3)\n",
    "decoded2 = Dense(1000, activation='relu')(decoded3)\n",
    "decoded1 = Dense(784, activation='sigmoid')(decoded2)\n",
    "autoencoder = Model(input=input_img, output=decoded1)\n",
    "encoder = Model(input=input_img, output=encoded3)\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train, nb_epoch=50, batch_size=25, shuffle=True, validation_data=(x_val, x_val), verbose=0)\n",
    "autoencoder.save('denoise_768x1000x500x250x16.h5')\n",
    "decoded_test = autoencoder.predict(x_test)\n",
    "error = error_reconstruccion(x_test, decoded_test)\n",
    "print (\"deep AE de 4 capas y 2 dimensiones\")\n",
    "print (\"Porcentaje de compresion: \" + str(float(target_dim)*100/784) +\" %\")\n",
    "print (\"Error de reconstruccion: \" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nplot=50 #warning: mind your memory!\n",
    "encoded_train = encoder.predict(x_train[:nplot])\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "encoded_train = model.fit_transform(encoded_train)\n",
    "colors={0:'b',1:'g',2:'r',3:'c',4:'m',5:'y',6:'k',7:'orange',8:'darkgreen',9:'maroon'}\n",
    "markers={0:'o',1:'+',2: 'v',3:'<',4:'>',5:'^',6:'s',7:'p',8:'*',9:'x'}\n",
    "plt.figure(figsize=(10, 10))\n",
    "for idx in range(0,nplot):\n",
    "    label = y_train[idx]\n",
    "    line = plt.plot(encoded_train[idx][0], encoded_train[idx][1], color=colors[label], marker=markers[label], markersize=6)\n",
    "    dae_train = encoder.predict(x_train)\n",
    "    # pca_train = pca.transform(x_train)\n",
    "    encoded_train = dae_train[:nplot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
