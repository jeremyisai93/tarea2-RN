{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta seccióon se utilizará un AE para pre-entrenar redes profundas. El efecto esperado es regularizar el modelo, posicionando el modelo de partida en una buena zona del espacio de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import binomial\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, load_model, save_model, Sequential\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Normalizacion de imagenes\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Transformación en vectores R^{784}\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Creación conjunto de validación\n",
    "nval = 5000\n",
    "x_val = x_train[-nval:]\n",
    "y_val = y_train[-nval:]\n",
    "x_train = x_train[:-nval]\n",
    "y_train = y_train[:-nval]\n",
    "\n",
    "# Transformación de salidas a probabilidades de activación\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_val = np_utils.to_categorical(y_val, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***a)*** Inicialmente se entrena una red FF para clasificar las imágenes de MNIST. Se emplea SGD básico con tasa de aprendizaje fija $\\eta = 1$ y 50 epochs. Se utiliza una arquitectura $784 \\times 1000 \\times 1000 \\times 10$ y funciones de activación sigmoidales. Se procede a determinar el error de clasificación alcanzado por el modelo en el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,010.0\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'sigmoid'\n",
    "decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer2 = 'sigmoid'\n",
    "decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)))\n",
    "model.add(Dense(n_hidden_layer2, activation=activation_layer2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train, epochs=epochs_, batch_size=25,shuffle=True, validation_data=(x_val, Y_val), verbose=0)\n",
    "model.save('entrenamientos/SigmoidNet784x1000x1000x10-50epochs.h5')\n",
    "#TRAINING CAN THEN BE RESUMED FROM THIS POINT :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9824/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bTest loss: 2.8421617866516113\n",
      "Test accuracy: 0.8227000014305115\n"
     ]
    }
   ],
   "source": [
    "model1 = load_model('entrenamientos/SigmoidNet784x1000x1000x10-50epochs.h5');\n",
    "model1.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "loss,acc = model1.evaluate(x_test, Y_test, verbose=1)\n",
    "print('Test loss: {0}'.format(loss)); print('Test accuracy: {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***b)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_3, outputs=sigmoid.0)`\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_3, outputs=sigmoid.0)`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e1376e410ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mencoder1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_img1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoded1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mautoencoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m autoencoder1.fit(X_train, X_train, nb_epoch=epochs_, batch_size=batch_size_, \n\u001b[0m\u001b[1;32m     19\u001b[0m                  shuffle=True, validation_data=(X_val, X_val))\n\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "## PARAMETERS\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'sigmoid'; decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer1 = 'sigmoid'; decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "### AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1, activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train, x_train, epochs=epochs_, batch_size=batch_size_, \n",
    "                 shuffle=True, validation_data=(x_val, x_val))\n",
    "\n",
    "### AUTOENCODER 2\n",
    "# FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_train_encoded1 = encoder1.predict(x_train) \n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer2, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_encoded1, x_train_encoded1, epochs=epochs_, batch_size=batch_size_,\n",
    "                 shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "\n",
    "### FINE TUNNING\n",
    "model = Sequential()\n",
    "model.add( Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)) )\n",
    "model.layers[-1].set_weights( autoencoder1.layers[1].get_weights() )\n",
    "model.add( Dense(n_hidden_layer2, activation=activation_layer2) )\n",
    "model.layers[-1].set_weights( autoencoder2.layers[1].get_weights() )\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# saving net before fine tunning\n",
    "save_keras_model(model, 'entrenamientos/mlp_768x1000x1000x10_pretrain_ae')\n",
    "model.fit(x_train, Y_train, epochs=20, batch_size=25, shuffle=True, validation_data=(x_val, Y_val))\n",
    "# saving net after fine tunning\n",
    "save_keras_model(model, 'entrenamientos/mlp_768x1000x1000x10_finetunning_ae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***c)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_noise(X_train, X_val, X_test, devst=0.1):\n",
    "    noise_mask = devst*np.random.standard_normal(size=X_train.shape)\n",
    "    X_train_noisy = X_train + noise_mask\n",
    "    noise_mask = devst*np.random.standard_normal(size=X_val.shape)\n",
    "    X_val_noisy = X_val + noise_mask\n",
    "    noise_mask = devst*np.random.standard_normal(size=X_test.shape)\n",
    "    X_test_noisy = X_test + noise_mask\n",
    "    return X_train_noisy, X_val_noisy, X_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_2, outputs=sigmoid.0)`\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_2, outputs=sigmoid.0)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      " 1950/55000 [>.............................] - ETA: 25s - loss: 0.3400\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "## PARAMETERS\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'sigmoid'; decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer1 = 'sigmoid'; decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "# Data with gaussian noise\n",
    "x_train_noisy, x_val_noisy, x_test_noisy = gaussian_noise(x_train, x_val, x_test)\n",
    "\n",
    "### DENOISING AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1, activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train_noisy, x_train, epochs=epochs_, batch_size=batch_size_, \n",
    "                 shuffle=True, validation_data=(x_val_noisy, x_val))\n",
    "\n",
    "### DENOISING AUTOENCODER 2\n",
    "# FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_train_encoded1 = encoder1.predict(x_train) \n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "# adding gaussian noise also to encoded data\n",
    "x_train_noisy_encoded1, x_val_noisy_encoded1, x_test_noisy_encoded1 = gaussian_noise(x_train_encoded1, x_val_encoded1,\n",
    "                                                                                     x_test_encoded1)\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer2, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_noisy_encoded1, x_train_encoded1, epochs=epochs_, batch_size=batch_size_,\n",
    "                 shuffle=True, validation_data=(x_val_noisy_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "\n",
    "### FINE TUNNING\n",
    "model = Sequential()\n",
    "model.add( Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)) )\n",
    "model.layers[-1].set_weights( autoencoder1.layers[1].get_weights() )\n",
    "model.add( Dense(n_hidden_layer2, activation=activation_layer2) )\n",
    "model.layers[-1].set_weights( autoencoder2.layers[1].get_weights() )\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# saving net before fine tunning\n",
    "save_keras_model(model, 'entrenamientos/mlp_768x1000x1000x10_pretrain_dae')\n",
    "model.fit(x_train, Y_train, epochs=20, batch_size=25, shuffle=True, validation_data=(x_val, Y_val))\n",
    "# saving net after fine tunning\n",
    "save_keras_model(model, 'entrenamientos/mlp_768x1000x1000x10_finetunning_dae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***d)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_i) Función de activación ReLu:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'relu'; decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer1 = 'relu'; decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "### AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1, activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train, x_train, epochs=epochs_, batch_size=batch_size_, \n",
    "                 shuffle=True, validation_data=(x_val, x_val))\n",
    "\n",
    "### AUTOENCODER 2\n",
    "# FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_train_encoded1 = encoder1.predict(x_train) \n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer2, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_encoded1, x_train_encoded1, epochs=epochs_, batch_size=batch_size_,\n",
    "                 shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "\n",
    "### FINE TUNNING\n",
    "model = Sequential()\n",
    "model.add( Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)) )\n",
    "model.layers[-1].set_weights( autoencoder1.layers[1].get_weights() )\n",
    "model.add( Dense(n_hidden_layer2, activation=activation_layer2) )\n",
    "model.layers[-1].set_weights( autoencoder2.layers[1].get_weights() )\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# saving net before fine tunning\n",
    "save_keras_model(model, 'entrenamientos/Relu_mlp_768x1000x1000x10_pretrain_ae')\n",
    "model.fit(x_train, Y_train, epochs=20, batch_size=25, shuffle=True, validation_data=(x_val, Y_val))\n",
    "# saving net after fine tunning\n",
    "save_keras_model(model, 'entrenamientos/Relu_mlp_768x1000x1000x10_finetunning_ae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ii) Función de activación Tanh:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'tanh'; decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer1 = 'tanh'; decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "### AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1, activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train, x_train, epochs=epochs_, batch_size=batch_size_, \n",
    "                 shuffle=True, validation_data=(x_val, x_val))\n",
    "\n",
    "### AUTOENCODER 2\n",
    "# FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_train_encoded1 = encoder1.predict(x_train) \n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer2, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_encoded1, x_train_encoded1, epochs=epochs_, batch_size=batch_size_,\n",
    "                 shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "\n",
    "### FINE TUNNING\n",
    "model = Sequential()\n",
    "model.add( Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)) )\n",
    "model.layers[-1].set_weights( autoencoder1.layers[1].get_weights() )\n",
    "model.add( Dense(n_hidden_layer2, activation=activation_layer2) )\n",
    "model.layers[-1].set_weights( autoencoder2.layers[1].get_weights() )\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# saving net before fine tunning\n",
    "save_keras_model(model, 'entrenamientos/Tanh_mlp_768x1000x1000x10_pretrain_ae')\n",
    "model.fit(x_train, Y_train, epochs=20, batch_size=25, shuffle=True, validation_data=(x_val, Y_val))\n",
    "# saving net after fine tunning\n",
    "save_keras_model(model, 'entrenamientos/Tanh_mlp_768x1000x1000x10_finetunning_ae')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
