{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pre-entrenamiento\n",
    "\n",
    "En esta seccióon se utilizará un AE para pre-entrenar redes profundas. El efecto esperado es regularizar el modelo, posicionando el modelo de partida en una buena zona del espacio de parámetros. Se hace uso de las siguientes librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import binomial\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, load_model, save_model, Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga y pre-procesa el data set de igual manera que en la secciones _3.1_ y _3.2_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Normalizacion de imagenes\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Transformación en vectores R^{784}\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Creación conjunto de validación\n",
    "nval = 5000\n",
    "x_val = x_train[-nval:]\n",
    "y_val = y_train[-nval:]\n",
    "x_train = x_train[:-nval]\n",
    "y_train = y_train[:-nval]\n",
    "\n",
    "# Transformación de salidas a probabilidades de activación\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_val = np_utils.to_categorical(y_val, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***a)*** Inicialmente se entrena una red FF para clasificar las imágenes de MNIST. Se emplea SGD básico con tasa de aprendizaje fija $\\eta = 1$ y 50 epochs. Se utiliza una arquitectura $784 \\times 1000 \\times 1000 \\times 10$ y funciones de activación sigmoidales. Se procede a determinar el error de clasificación alcanzado por el modelo en el conjunto de test, sin el empleo de pre-entrenamiento por medio de AE's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,010\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.1519 - acc: 0.9492 - val_loss: 0.0508 - val_acc: 0.9834\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0581 - acc: 0.9803 - val_loss: 0.0382 - val_acc: 0.9874\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0481 - acc: 0.9839 - val_loss: 0.0345 - val_acc: 0.9888\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0415 - acc: 0.9860 - val_loss: 0.0301 - val_acc: 0.9902\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.0369 - acc: 0.9875 - val_loss: 0.0249 - val_acc: 0.9923\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0327 - acc: 0.9891 - val_loss: 0.0252 - val_acc: 0.9917\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0293 - acc: 0.9902 - val_loss: 0.0216 - val_acc: 0.9928\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0266 - acc: 0.9912 - val_loss: 0.0194 - val_acc: 0.9936\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0242 - acc: 0.9917 - val_loss: 0.0190 - val_acc: 0.9941\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0189 - val_acc: 0.9942\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0203 - acc: 0.9933 - val_loss: 0.0182 - val_acc: 0.9941\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0187 - acc: 0.9938 - val_loss: 0.0198 - val_acc: 0.9935\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0173 - acc: 0.9943 - val_loss: 0.0171 - val_acc: 0.9945\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0161 - acc: 0.9947 - val_loss: 0.0154 - val_acc: 0.9949\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0151 - acc: 0.9949 - val_loss: 0.0163 - val_acc: 0.9944\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0141 - acc: 0.9954 - val_loss: 0.0138 - val_acc: 0.9952\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.0132 - acc: 0.9956 - val_loss: 0.0136 - val_acc: 0.9955\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0123 - acc: 0.9960 - val_loss: 0.0138 - val_acc: 0.9955\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0130 - val_acc: 0.9955\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0132 - val_acc: 0.9956\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0137 - val_acc: 0.9958\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0095 - acc: 0.9969 - val_loss: 0.0123 - val_acc: 0.9960\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0088 - acc: 0.9971 - val_loss: 0.0122 - val_acc: 0.9963\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0126 - val_acc: 0.9960\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0141 - val_acc: 0.9956\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0072 - acc: 0.9977 - val_loss: 0.0117 - val_acc: 0.9962\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0120 - val_acc: 0.9961\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0114 - val_acc: 0.9966\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0117 - val_acc: 0.9964\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0056 - acc: 0.9982 - val_loss: 0.0114 - val_acc: 0.9966\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0118 - val_acc: 0.9964\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0124 - val_acc: 0.9962\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0107 - val_acc: 0.9967\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0125 - val_acc: 0.9964\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 31s - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0121 - val_acc: 0.9962\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0116 - val_acc: 0.9966\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0128 - val_acc: 0.9965\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0121 - val_acc: 0.9965\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0107 - val_acc: 0.9969\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0115 - val_acc: 0.9967\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0117 - val_acc: 0.9965\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0116 - val_acc: 0.9966\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0118 - val_acc: 0.9966\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0020 - acc: 0.9996 - val_loss: 0.0118 - val_acc: 0.9966\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0114 - val_acc: 0.9966\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0119 - val_acc: 0.9965\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0115 - val_acc: 0.9967\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0121 - val_acc: 0.9966\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 29s - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0114 - val_acc: 0.9967\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 31s - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0122 - val_acc: 0.9966\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'sigmoid'\n",
    "decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer2 = 'sigmoid'\n",
    "decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)))\n",
    "model.add(Dense(n_hidden_layer2, activation=activation_layer2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train, epochs=epochs_, batch_size=25,shuffle=True, validation_data=(x_val, Y_val), verbose=1)\n",
    "model.save('entrenamientos/SigmoidNet784x1000x1000x10-50epochs.h5')\n",
    "#TRAINING CAN THEN BE RESUMED FROM THIS POINT :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.01269852604996704\n",
      "Test accuracy: 0.9960399953842163\n",
      "Error de clasificacion en conjunto de test: 0.396000461578 %\n"
     ]
    }
   ],
   "source": [
    "model1 = load_model('entrenamientos/SigmoidNet784x1000x1000x10-50epochs.h5');\n",
    "model1.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "loss,acc = model1.evaluate(x_test, Y_test, verbose=0)\n",
    "print('Test loss: {0}'.format(loss)); print('Test accuracy: {0}'.format(acc))\n",
    "print ('Error de clasificacion en conjunto de test:', (1 - acc) * 100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se determina un error de clasificación alcanzado por el modelo en el conjunto de test igual a $0.396000461578\\%$. Resulta ser un error bastante competitivo, implicando un desafío considerable de superar (experimentación realizada en ítems posteriores) por modelos pre-entrenados con AE's.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***b)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.2367 - val_loss: 0.1992\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1797 - val_loss: 0.1649\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1557 - val_loss: 0.1475\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1413 - val_loss: 0.1357\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1312 - val_loss: 0.1271\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1236 - val_loss: 0.1205\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1176 - val_loss: 0.1152\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1128 - val_loss: 0.1109\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1088 - val_loss: 0.1073\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1054 - val_loss: 0.1043\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1026 - val_loss: 0.1017\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.1001 - val_loss: 0.0994\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0980 - val_loss: 0.0974\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0961 - val_loss: 0.0957\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0944 - val_loss: 0.0941\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0929 - val_loss: 0.0927\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0915 - val_loss: 0.0915\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0903 - val_loss: 0.0903\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0892 - val_loss: 0.0893\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0882 - val_loss: 0.0883\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0873 - val_loss: 0.0875\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0864 - val_loss: 0.0867\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0856 - val_loss: 0.0859\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0849 - val_loss: 0.0852\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0842 - val_loss: 0.0846\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0836 - val_loss: 0.0840\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0830 - val_loss: 0.0834\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0825 - val_loss: 0.0829\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0819 - val_loss: 0.0824\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0815 - val_loss: 0.0819\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0810 - val_loss: 0.0815\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0806 - val_loss: 0.0811\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0802 - val_loss: 0.0807\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0798 - val_loss: 0.0803\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0794 - val_loss: 0.0799\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0790 - val_loss: 0.0796\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 27s - loss: 0.0787 - val_loss: 0.0793\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0784 - val_loss: 0.0790\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0781 - val_loss: 0.0787\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0778 - val_loss: 0.0784\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0775 - val_loss: 0.0781\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0772 - val_loss: 0.0779\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0770 - val_loss: 0.0776\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0767 - val_loss: 0.0774\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0765 - val_loss: 0.0772\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0763 - val_loss: 0.0769\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0760 - val_loss: 0.0767\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0758 - val_loss: 0.0765\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0756 - val_loss: 0.0763\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 26s - loss: 0.0754 - val_loss: 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "c:\\python35\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.6675 - val_loss: 0.6460\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.6297 - val_loss: 0.6169\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.6082 - val_loss: 0.6004\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5947 - val_loss: 0.5898\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5859 - val_loss: 0.5826\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5797 - val_loss: 0.5775\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 31s - loss: 0.5754 - val_loss: 0.5739\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 31s - loss: 0.5722 - val_loss: 0.5713\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 31s - loss: 0.5699 - val_loss: 0.5692\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5680 - val_loss: 0.5676\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5666 - val_loss: 0.5663\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5655 - val_loss: 0.5653\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5645 - val_loss: 0.5645\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5638 - val_loss: 0.5638\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5631 - val_loss: 0.5632\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5626 - val_loss: 0.5627\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5621 - val_loss: 0.5623\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5618 - val_loss: 0.5619\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5614 - val_loss: 0.5616\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5611 - val_loss: 0.5613\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5609 - val_loss: 0.5611\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5606 - val_loss: 0.5609\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5604 - val_loss: 0.5606\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5602 - val_loss: 0.5605\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5600 - val_loss: 0.5603\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5598 - val_loss: 0.5601\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5597 - val_loss: 0.5600\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5595 - val_loss: 0.5598\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5594 - val_loss: 0.5597\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5593 - val_loss: 0.5596\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5591 - val_loss: 0.5594\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5590 - val_loss: 0.5593\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5589 - val_loss: 0.5592\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5588 - val_loss: 0.5591\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5587 - val_loss: 0.5590\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5586 - val_loss: 0.5590\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5585 - val_loss: 0.5589\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5585 - val_loss: 0.5588\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5584 - val_loss: 0.5587\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5583 - val_loss: 0.5586\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5582 - val_loss: 0.5586\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5582 - val_loss: 0.5585\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 31s - loss: 0.5581 - val_loss: 0.5585\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5581 - val_loss: 0.5584\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5580 - val_loss: 0.5583\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5579 - val_loss: 0.5583\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5579 - val_loss: 0.5582\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 30s - loss: 0.5578 - val_loss: 0.5582\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 31s - loss: 0.5578 - val_loss: 0.5581\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 31s - loss: 0.5577 - val_loss: 0.5581\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_keras_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8209a4c4bfbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# saving net before fine tunning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0msave_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'entrenamientos/mlp_768x1000x1000x10_pretrain_ae'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# saving net after fine tunning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_keras_model' is not defined"
     ]
    }
   ],
   "source": [
    "## PARAMETERS\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'sigmoid'; decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer2 = 'sigmoid'; decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "### AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1, activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train, x_train, epochs=epochs_, batch_size=batch_size_, \n",
    "                 shuffle=True, validation_data=(x_val, x_val))\n",
    "autoencoder1.save('entrenamientos/3_3/Sigmoid_autoencoder_layer1_ae.h5')\n",
    "encoder1.save('entrenamientos/3_3/Sigmoid_encoder_layer1_ae.h5')\n",
    "\n",
    "### AUTOENCODER 2\n",
    "# FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_train_encoded1 = encoder1.predict(x_train) \n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer1, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_encoded1, x_train_encoded1, epochs=epochs_, batch_size=batch_size_,\n",
    "                 shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "autoencoder2.save('entrenamientos/3_3/Sigmoid_autoencoder_layer2_ae.h5')\n",
    "encoder2.save('entrenamientos/3_3/Sigmoid_encoder_layer2_ae.h5')\n",
    "\n",
    "### FINE TUNNING\n",
    "model = Sequential()\n",
    "model.add( Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)) )\n",
    "model.layers[-1].set_weights( autoencoder1.layers[1].get_weights() )\n",
    "model.add( Dense(n_hidden_layer2, activation=activation_layer2) )\n",
    "model.layers[-1].set_weights( autoencoder2.layers[1].get_weights() )\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train, epochs=20, batch_size=25, shuffle=True, validation_data=(x_val, Y_val))\n",
    "model.save('entrenamientos/3_3/Sigmoid_784x1000x1000x10_finetunning_ae.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***c)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_noise(X_train, X_val, X_test, devst=0.1):\n",
    "    noise_mask = devst*np.random.standard_normal(size=X_train.shape)\n",
    "    X_train_noisy = X_train + noise_mask\n",
    "    noise_mask = devst*np.random.standard_normal(size=X_val.shape)\n",
    "    X_val_noisy = X_val + noise_mask\n",
    "    noise_mask = devst*np.random.standard_normal(size=X_test.shape)\n",
    "    X_test_noisy = X_test + noise_mask\n",
    "    return X_train_noisy, X_val_noisy, X_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'sigmoid'; decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer2 = 'sigmoid'; decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "# Data with gaussian noise\n",
    "x_train_noisy, x_val_noisy, x_test_noisy = gaussian_noise(x_train, x_val, x_test)\n",
    "\n",
    "### DENOISING AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1, activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train_noisy, x_train, epochs=epochs_, batch_size=batch_size_, \n",
    "                 shuffle=True, validation_data=(x_val_noisy, x_val))\n",
    "autoencoder1.save('entrenamientos/3_3/Sigmoid_autoencoder_layer1_dae.h5')\n",
    "encoder1.save('entrenamientos/3_3/Sigmoid_encoder_layer1_dae.h5')\n",
    "\n",
    "### DENOISING AUTOENCODER 2\n",
    "# FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_train_encoded1 = encoder1.predict(x_train) \n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "# adding gaussian noise also to encoded data\n",
    "x_train_noisy_encoded1, x_val_noisy_encoded1, x_test_noisy_encoded1 = gaussian_noise(x_train_encoded1, x_val_encoded1,\n",
    "                                                                                     x_test_encoded1)\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer1, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_noisy_encoded1, x_train_encoded1, epochs=epochs_, batch_size=batch_size_,\n",
    "                 shuffle=True, validation_data=(x_val_noisy_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "autoencoder2.save('entrenamientos/3_3/Sigmoid_autoencoder_layer2_dae.h5')\n",
    "encoder2.save('entrenamientos/3_3/Sigmoid_encoder_layer2_dae.h5')\n",
    "\n",
    "### FINE TUNNING\n",
    "model = Sequential()\n",
    "model.add( Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)) )\n",
    "model.layers[-1].set_weights( autoencoder1.layers[1].get_weights() )\n",
    "model.add( Dense(n_hidden_layer2, activation=activation_layer2) )\n",
    "model.layers[-1].set_weights( autoencoder2.layers[1].get_weights() )\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train, epochs=20, batch_size=25, shuffle=True, validation_data=(x_val, Y_val))\n",
    "model.save('entrenamientos/3_3/Sigmoid_784x1000x1000x10_finetunning_dae.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***d)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_i) Función de activación ReLu:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'relu'; decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer2 = 'relu'; decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "### AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1, activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train, x_train, epochs=epochs_, batch_size=batch_size_, \n",
    "                 shuffle=True, validation_data=(x_val, x_val))\n",
    "autoencoder1.save('entrenamientos/3_3/Relu_autoencoder_layer1_ae.h5')\n",
    "encoder1.save('entrenamientos/3_3/Relu_encoder_layer1_ae.h5')\n",
    "\n",
    "### AUTOENCODER 2\n",
    "# FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_train_encoded1 = encoder1.predict(x_train) \n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer1, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_encoded1, x_train_encoded1, epochs=epochs_, batch_size=batch_size_,\n",
    "                 shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "autoencoder2.save('entrenamientos/3_3/Relu_autoencoder_layer2_ae.h5')\n",
    "encoder2.save('entrenamientos/3_3/Relu_encoder_layer2_ae.h5')\n",
    "\n",
    "### FINE TUNNING\n",
    "model = Sequential()\n",
    "model.add( Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)) )\n",
    "model.layers[-1].set_weights( autoencoder1.layers[1].get_weights() )\n",
    "model.add( Dense(n_hidden_layer2, activation=activation_layer2) )\n",
    "model.layers[-1].set_weights( autoencoder2.layers[1].get_weights() )\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train, epochs=20, batch_size=25, shuffle=True, validation_data=(x_val, Y_val))\n",
    "model.save('entrenamientos/3_3/Relu_784x1000x1000x10_finetunning_ae.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ii) Función de activación Tanh:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'tanh'; decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer2 = 'tanh'; decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50\n",
    "batch_size_ = 25\n",
    "\n",
    "### AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1, activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train, x_train, epochs=epochs_, batch_size=batch_size_, \n",
    "                 shuffle=True, validation_data=(x_val, x_val))\n",
    "autoencoder1.save('entrenamientos/3_3/Tanh_autoencoder_layer1_ae.h5')\n",
    "encoder1.save('entrenamientos/3_3/Tanh_encoder_layer1_ae.h5')\n",
    "\n",
    "### AUTOENCODER 2\n",
    "# FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_train_encoded1 = encoder1.predict(x_train) \n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer1, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_encoded1, x_train_encoded1, epochs=epochs_, batch_size=batch_size_,\n",
    "                 shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "autoencoder2.save('entrenamientos/3_3/Tanh_autoencoder_layer2_ae.h5')\n",
    "encoder2.save('entrenamientos/3_3/Tanh_encoder_layer2_ae.h5')\n",
    "\n",
    "### FINE TUNNING\n",
    "model = Sequential()\n",
    "model.add( Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)) )\n",
    "model.layers[-1].set_weights( autoencoder1.layers[1].get_weights() )\n",
    "model.add( Dense(n_hidden_layer2, activation=activation_layer2) )\n",
    "model.layers[-1].set_weights( autoencoder2.layers[1].get_weights() )\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=optimizer_, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train, epochs=20, batch_size=25, shuffle=True, validation_data=(x_val, Y_val))\n",
    "model.save('entrenamientos/3_3/Tanh_784x1000x1000x10_finetunning_ae.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
